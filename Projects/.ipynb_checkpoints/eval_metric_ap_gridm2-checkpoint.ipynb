{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188733f4-567a-43ca-a309-8c23e3266769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from data import ImageDetectionsField, TextField, RawField\n",
    "from data import COCO, DataLoader\n",
    "from data.dataset import AP_Dataset, APeval_Dataset, SA_Dataset, SAeval_Dataset\n",
    "import evaluation\n",
    "from evaluation import PTBTokenizer, Cider\n",
    "# from models.transformer import Transformer, MemoryAugmentedEncoder, MeshedDecoder, ScaledDotProductAttentionMemory\n",
    "from models.grid_m2 import Transformer, TransformerEncoder, MeshedDecoder, ScaledDotProductAttentionMemory, ScaledDotProductAttention\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn import NLLLoss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import argparse, os, pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from shutil import copyfile\n",
    "import h5py\n",
    "from utils import text_progress2, text_progress\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ee104-c363-473a-8c6c-56105168407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, dataloader, text_field, mode=\"multiple\", is_sample=False, beam_size=5, top_k=5, top_p=0.8):\n",
    "    import itertools\n",
    "    print(dataloader)\n",
    "    print(mode)\n",
    "    model.eval()\n",
    "    gen = {}\n",
    "    gts = {}\n",
    "    with tqdm(desc='evalulateion metrics', unit='it', total=len(dataloader)) as pbar:\n",
    "        for it, batch in enumerate(iter(dataloader)):\n",
    "            images = batch['roi_feat']\n",
    "            caps_gt = batch['cap']\n",
    "            images = images.to(device)\n",
    "            with torch.no_grad():\n",
    "#                 beam_size = 5\n",
    "                out, _ = model.beam_search(images, 20, text_field.vocab.stoi['<eos>'], beam_size, out_size=1, is_sample=is_sample, top_k=5, top_p=0.8)\n",
    "#                 if decode == \"beam_search\":\n",
    "#                     out, _ = model.beam_search(images, 20, text_field.vocab.stoi['<eos>'], 5, out_size=1, is_sample=False)\n",
    "#                 elif decode == \"top-k_sampling\":\n",
    "#                     out, _ = model.beam_search(images, 20, text_field.vocab.stoi['<eos>'], 1, out_size=1, is_sample=True)\n",
    "            caps_gen = text_field.decode(out, join_words=False)\n",
    "            for i, (gts_i, gen_i) in enumerate(zip(caps_gt, caps_gen)):\n",
    "                gen_i = ' '.join([k for k, g in itertools.groupby(gen_i)])\n",
    "                gen['%d_%d' % (it, i)] = [gen_i, ]\n",
    "                if mode == \"multiple\":\n",
    "                    gts['%d_%d' % (it, i)] = gts_i\n",
    "                elif mode == \"single\":\n",
    "                    gts['%d_%d' % (it, i)] = [gts_i[0]]\n",
    "            pbar.update()\n",
    "\n",
    "    gts = evaluation.PTBTokenizer.tokenize(gts)\n",
    "    gen = evaluation.PTBTokenizer.tokenize(gen)\n",
    "    scores, _ = evaluation.compute_scores(gts, gen, spice=False)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb3c8e-c055-4ac0-aa70-4a312b25b17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0b12d-fa33-4443-9498-dbc3aee9d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2700e21-55c7-42ee-9268-75558e696a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "text_field = TextField(init_token='<bos>', eos_token='<eos>', lower=True, tokenize='spacy',\n",
    "                       remove_punctuation=True, nopoints=False)\n",
    "text_field.vocab = pickle.load(open('vocab.pkl', 'rb'))\n",
    "\n",
    "encoder = TransformerEncoder(3, 0, attention_module=ScaledDotProductAttention, attention_module_kwargs={'m': 40})\n",
    "decoder = MeshedDecoder(len(text_field.vocab), 54, 3, text_field.vocab.stoi['<pad>'])\n",
    "model = Transformer(text_field.vocab.stoi['<bos>'], encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6640d7-492b-4377-a09b-29e94979691a",
   "metadata": {},
   "source": [
    "## 1. artpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cf6de3-3cfd-4dac-a1d6-d64ec41fdad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_myidx = np.load('../Dataset/artpedia/train_myidx.npy')\n",
    "val_myidx = np.load('../Dataset/artpedia/val_myidx.npy')\n",
    "test_myidx = np.load('../Dataset/artpedia/test_myidx.npy')\n",
    "\n",
    "ap_train_dataset = h5py.File(\"../Dataset/artpedia/ap_train_grid.hdf5\", \"r\")\n",
    "ap_val_dataset = h5py.File(\"../Dataset/artpedia/ap_val_grid.hdf5\", \"r\")\n",
    "ap_test_dataset = h5py.File(\"../Dataset/artpedia/ap_test_grid.hdf5\", \"r\")\n",
    "print(\"loading data: done!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488f8c01-5e00-42d0-8a96-ee2380fac9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artpedia dataset\n",
    "dict_artpedia_test = APeval_Dataset(ap_test_dataset, test_myidx, text_field, max_detections=49, lower=True, remove_punctuation=True, tokenize='spacy')\n",
    "\n",
    "# artpedia, dataloader\n",
    "dict_artpedia_test_data_loader = TorchDataLoader(dict_artpedia_test, batch_size=50, collate_fn=lambda x: text_progress2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1bc509-fce3-4517-aa44-d3e14147af94",
   "metadata": {},
   "source": [
    "#### 1.1 artpedia, one caption for evaluation, beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75c5b3-306a-4b61-bc43-a250dcba5de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch model\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode = \"single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b45e1-bfea-4aca-956c-eebf95ccdea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without fine-tuning, off-the-shelf model\n",
    "data = torch.load('meshed_memory_transformer.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea105a5e-62bd-464d-be30-1c4c119a600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on artpedia, one image with one caption for training\n",
    "data = torch.load('saved_models/artpedia_finetune_singlecap.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81142695-0557-4842-b6cd-e2a4a15bb0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on artpedia, one image with multiple captions for training\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a82b9e6-8f61-450e-92e0-f32af0a82cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on artpedia, one image with multiple captions for training   + shuffle\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee71940-0004-4a80-a145-46fb20ac0fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search.   remove <unk> token\n",
    "# fine-tune on artpedia, one image with multiple captions for training   + shuffle\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf010ed9-db61-4456-a878-f7f005e043cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504081d-3a13-44de-bc1d-e09fbf6c52ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc052f-94d8-447c-afca-c58129550c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on artpedia, one image with multiple captions for training   + shuffle\n",
    "# generation by top-k sampling    k=5\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='single', is_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb308f-5489-45e8-850d-8749a9394e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf5f1b8-d0c1-4ade-905a-51010c454f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on artpedia, one image with multiple captions for training   + shuffle\n",
    "# generation by top-k sampling    k=10\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='single', decode=\"top-k_sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150315d5-7205-4f09-9330-f1ff42f9a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on artpedia, one image with multiple captions for training   + shuffle\n",
    "# generation by top-k sampling    k=10\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='single', is_sample=True, top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c7c6f-b02a-427e-ab1f-312a5ecaebea",
   "metadata": {},
   "source": [
    "#### 1.2 artpedia, multiple captions for evaluation, beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a8a17b-2cb3-4d78-a169-fc5b2cefd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** grid_m2\n",
    "# without fine-tuning, off-the-shelf model\n",
    "data = torch.load('saved_models_grid_m2/grid_m2_best.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a3825-0fd6-4bcc-be54-e7697234b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on artpedia, one image with one caption for training\n",
    "data = torch.load('saved_models/artpedia_finetune_singlecap.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df9eb92-8f5b-475f-b55b-5f27f7443551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcfa966-545c-4d34-932f-3f93451ec601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** grid_m2\n",
    "# fine-tune on artpedia, one image with multiple captions for training\n",
    "data = torch.load('saved_models/grid_m2_tr_last_15epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960b58e-52ca-4c41-92ab-0298ef20a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on artpedia, one image with multiple captions for training\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c7f89-3625-4677-a1f2-ae64d2a85ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on artpedia, one image with multiple captions for training      shuffle\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfac020-93e3-4a39-8809-d88e635b088c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d42a19-ff8c-4647-bc54-d3e765365d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search   remove unk\n",
    "# fine-tune on artpedia, one image with multiple captions for training      shuffle\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, is_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6319a989-ab7c-4fbb-91f3-12cc5df26a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** grid_m2\n",
    "# beam search   remove unk\n",
    "# fine-tune on artpedia, one image with multiple captions for training      shuffle\n",
    "data = torch.load('saved_models/grid_m2_tr_last_15epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, is_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bfe0e8-bbed-4c5e-8886-c19de592dc24",
   "metadata": {},
   "source": [
    "## 2. semart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e796e9b-2e29-4f0b-9987-4b5cdbf7cfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63624600-04f8-419c-b8f0-10c6942c8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_test_csv = pd.read_csv(\"../Dataset/SemArt/prediction_csvs/semart_test_prediction.csv\")\n",
    "sa_test_csv = sa_test_csv[sa_test_csv['predictioin']==0]\n",
    "test_roi_feats = h5py.File(\"../Dataset/SemArt/sa_test_grid.hdf5\", \"r\")\n",
    "test_img_names = np.unique(sa_test_csv['img_name'].to_numpy())\n",
    "test_img_caps_map = json.load(open('../Dataset/SemArt/test_img_caps_map.json'))\n",
    "\n",
    "dict_semart_test = SAeval_Dataset(sa_test_csv, test_img_names, test_img_caps_map, test_roi_feats, text_field, max_detections=49, lower=True, remove_punctuation=True, tokenize='spacy')\n",
    "dict_semart_test_data_loader = TorchDataLoader(dict_semart_test, batch_size=10,\n",
    "                                  collate_fn=lambda x: text_progress2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89f439-5d51-4d68-906f-6333e189342e",
   "metadata": {},
   "source": [
    "#### 2.1 semart, multiple captions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92274535-f856-404d-a5ed-04165fc33bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# off-the-shelf model  ***\n",
    "data = torch.load('saved_models/saved_models_grid_m2/grid_m2_best.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b17a10-ed6f-425b-8ec9-29be384498e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac3d9dd-b0c1-4986-a942-f9e7cd2c0500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on SemArt\n",
    "data = torch.load('saved_models/saved_models_saft_grid_m2/sa_gridm2_sa_last_10epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d150c2-0d6a-466e-aa4b-1703e753eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on SemArt\n",
    "data = torch.load('saved_models/saved_models_saft_grid_m2/sa_gridm2_sa_last_13epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d24df-31a0-4796-a8a0-cc56407d192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on SemArt\n",
    "data = torch.load('saved_models/saved_models_saft_grid_m2/sa_gridm2_sa_last_12epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90b6035-2b9e-4e08-8100-5c4548725ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on SemArt\n",
    "data = torch.load('saved_models/saved_models_saft_grid_m2/sa_gridm2_sa_last_11epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22135f23-4e76-4463-9fbb-090d1495fc1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8f2c9-708c-41a5-8d14-378c01877a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d53db9-c8b3-438e-b74c-7a48761df13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on SemArt     remove <unk>\n",
    "data = torch.load('saved_models/saved_models_saft_grid_m2/sa_gridm2_sa_last_11epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd6a7c8-5117-42d8-b193-893afb6e7e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on SemArt     remove <unk>\n",
    "data = torch.load('saved_models/saved_models_saft_grid_m2/sa_gridm2_sa_last_10epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad690e9f-111e-4a90-a392-b09fb0e461d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on SemArt     remove <unk>\n",
    "data = torch.load('saved_models/saved_models_saft_grid_m2/sa_gridm2_sa_last_12epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825bd89-cf2c-4d04-b8d9-278b7ca50e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on SemArt     remove <unk>\n",
    "data = torch.load('saved_models/saved_models_saft_grid_m2/sa_gridm2_sa_last_13epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da88414d-b496-4ba9-b7b6-1221a71f956f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
