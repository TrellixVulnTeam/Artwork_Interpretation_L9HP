{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "188733f4-567a-43ca-a309-8c23e3266769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from data import ImageDetectionsField, TextField, RawField\n",
    "from data import COCO, DataLoader\n",
    "from data.dataset import AP_Dataset, APeval_Dataset, SA_Dataset, SAeval_Dataset\n",
    "import evaluation\n",
    "from evaluation import PTBTokenizer, Cider\n",
    "# from models.transformer import Transformer, MemoryAugmentedEncoder, MeshedDecoder, ScaledDotProductAttentionMemory\n",
    "from models.grid_m2_rst import  Transformer, TransformerEncoder, MeshedDecoder, ScaledDotProductAttention\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn import NLLLoss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import argparse, os, pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from shutil import copyfile\n",
    "import h5py\n",
    "from utils import text_progress2, text_progress\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a65ee104-c363-473a-8c6c-56105168407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, dataloader, text_field, mode=\"multiple\", is_sample=False, beam_size=5, top_k=5, top_p=0.8):\n",
    "    import itertools\n",
    "    print(dataloader)\n",
    "    print(mode)\n",
    "    model.eval()\n",
    "    gen = {}\n",
    "    gts = {}\n",
    "    with tqdm(desc='evalulateion metrics', unit='it', total=len(dataloader)) as pbar:\n",
    "        for it, batch in enumerate(iter(dataloader)):\n",
    "            images = batch['roi_feat']\n",
    "            caps_gt = batch['cap']\n",
    "            images = images.to(device)\n",
    "            with torch.no_grad():\n",
    "#                 beam_size = 5\n",
    "                out, _ = model.beam_search(images, 20, text_field.vocab.stoi['<eos>'], beam_size, out_size=1, is_sample=is_sample, top_k=5, top_p=0.8)\n",
    "#                 if decode == \"beam_search\":\n",
    "#                     out, _ = model.beam_search(images, 20, text_field.vocab.stoi['<eos>'], 5, out_size=1, is_sample=False)\n",
    "#                 elif decode == \"top-k_sampling\":\n",
    "#                     out, _ = model.beam_search(images, 20, text_field.vocab.stoi['<eos>'], 1, out_size=1, is_sample=True)\n",
    "            caps_gen = text_field.decode(out, join_words=False)\n",
    "            for i, (gts_i, gen_i) in enumerate(zip(caps_gt, caps_gen)):\n",
    "                gen_i = ' '.join([k for k, g in itertools.groupby(gen_i)])\n",
    "                gen['%d_%d' % (it, i)] = [gen_i, ]\n",
    "                if mode == \"multiple\":\n",
    "                    gts['%d_%d' % (it, i)] = gts_i\n",
    "                elif mode == \"single\":\n",
    "                    gts['%d_%d' % (it, i)] = [gts_i[0]]\n",
    "            pbar.update()\n",
    "\n",
    "    gts = evaluation.PTBTokenizer.tokenize(gts)\n",
    "    gen = evaluation.PTBTokenizer.tokenize(gen)\n",
    "    scores, _ = evaluation.compute_scores(gts, gen, spice=False)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb3c8e-c055-4ac0-aa70-4a312b25b17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61f0b12d-fa33-4443-9498-dbc3aee9d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2700e21-55c7-42ee-9268-75558e696a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "text_field = TextField(init_token='<bos>', eos_token='<eos>', lower=True, tokenize='spacy',\n",
    "                       remove_punctuation=True, nopoints=False)\n",
    "text_field.vocab = pickle.load(open('vocab.pkl', 'rb'))\n",
    "\n",
    "encoder = TransformerEncoder(3, 0, attention_module=ScaledDotProductAttention, attention_module_kwargs={'m': 40})\n",
    "decoder = MeshedDecoder(len(text_field.vocab), 54, 3, text_field.vocab.stoi['<pad>'])\n",
    "model = Transformer(text_field.vocab.stoi['<bos>'], encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6640d7-492b-4377-a09b-29e94979691a",
   "metadata": {},
   "source": [
    "## 1. artpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7cf6de3-3cfd-4dac-a1d6-d64ec41fdad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data: done!!!\n"
     ]
    }
   ],
   "source": [
    "train_myidx = np.load('../Dataset/artpedia/train_myidx.npy')\n",
    "val_myidx = np.load('../Dataset/artpedia/val_myidx.npy')\n",
    "test_myidx = np.load('../Dataset/artpedia/test_myidx.npy')\n",
    "\n",
    "ap_train_dataset = h5py.File(\"../Dataset/artpedia/ap_train_grid.hdf5\", \"r\")\n",
    "ap_val_dataset = h5py.File(\"../Dataset/artpedia/ap_val_grid.hdf5\", \"r\")\n",
    "ap_test_dataset = h5py.File(\"../Dataset/artpedia/ap_test_grid.hdf5\", \"r\")\n",
    "print(\"loading data: done!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "488f8c01-5e00-42d0-8a96-ee2380fac9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artpedia dataset\n",
    "dict_artpedia_test = APeval_Dataset(ap_test_dataset, test_myidx, text_field, max_detections=49, feature_type=\"grid\", lower=True, remove_punctuation=True, tokenize='spacy')\n",
    "\n",
    "# artpedia, dataloader\n",
    "dict_artpedia_test_data_loader = TorchDataLoader(dict_artpedia_test, batch_size=50, collate_fn=lambda x: text_progress2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1bc509-fce3-4517-aa44-d3e14147af94",
   "metadata": {},
   "source": [
    "#### 1.1 artpedia, one caption for evaluation, beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c7c6f-b02a-427e-ab1f-312a5ecaebea",
   "metadata": {},
   "source": [
    "#### 1.2 artpedia, multiple captions for evaluation, beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a8a17b-2cb3-4d78-a169-fc5b2cefd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** grid_m2rst\n",
    "# without fine-tuning, off-the-shelf model\n",
    "data = torch.load('saved_models_grid_m2rst/grid_m2_rst_best.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a86c41-9c3f-4ffb-9486-c246083b9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** grid_m2rst\n",
    "# without fine-tuning, off-the-shelf model\n",
    "data = torch.load('saved_models_grid_m2rst/grid_m2_rst_last.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe49209f-fa21-4513-9674-362f40300d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** grid_m2rst\n",
    "# without fine-tuning, off-the-shelf model\n",
    "data = torch.load('saved_models_grid_m2rst/grid_m2_rst_epoch41.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed8a3825-0fd6-4bcc-be54-e7697234b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on artpedia, one image with one caption for training\n",
    "data = torch.load('saved_models/artpedia_finetune_singlecap.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df9eb92-8f5b-475f-b55b-5f27f7443551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dcfa966-545c-4d34-932f-3f93451ec601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** grid_m2rst\n",
    "# fine-tune on artpedia, one image with multiple captions for training\n",
    "data = torch.load('saved_models/grid_m2_tr_last_18epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1960b58e-52ca-4c41-92ab-0298ef20a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on artpedia, one image with multiple captions for training\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e9c7f89-3625-4677-a1f2-ae64d2a85ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on artpedia, one image with multiple captions for training      shuffle\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfac020-93e3-4a39-8809-d88e635b088c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5d42a19-ff8c-4647-bc54-d3e765365d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search   remove unk\n",
    "# fine-tune on artpedia, one image with multiple captions for training      shuffle\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, is_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1b4e44e-eef0-482c-aafb-55daa12bb25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f2055ff5588>\n",
      "multiple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 7/7 [00:12<00:00,  1.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.3609881069135,\n",
       "  0.17902517899338843,\n",
       "  0.09118977721377369,\n",
       "  0.050181107544762536],\n",
       " 'METEOR': 0.09133971685517936,\n",
       " 'ROUGE': 0.23398104425547456,\n",
       " 'CIDEr': 0.09330209254997349}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# beam search   remove unk\n",
    "# fine-tune on artpedia, one image with multiple captions for training      shuffle\n",
    "data = torch.load('saved_models_grid_m2rst_apft/grid_m2_tr_last_18epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, is_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bfe0e8-bbed-4c5e-8886-c19de592dc24",
   "metadata": {},
   "source": [
    "## 2. semart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63624600-04f8-419c-b8f0-10c6942c8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_test_csv = pd.read_csv(\"../Dataset/SemArt/prediction_csvs/semart_test_prediction.csv\")\n",
    "sa_test_csv = sa_test_csv[sa_test_csv['predictioin']==0]\n",
    "test_roi_feats = h5py.File(\"../Dataset/SemArt/sa_test_grid.hdf5\", \"r\")\n",
    "test_img_names = np.unique(sa_test_csv['img_name'].to_numpy())\n",
    "test_img_caps_map = json.load(open('../Dataset/SemArt/test_img_caps_map.json'))\n",
    "\n",
    "dict_semart_test = SAeval_Dataset(sa_test_csv, test_img_names, test_img_caps_map, test_roi_feats, text_field, max_detections=49, lower=True, remove_punctuation=True, tokenize='spacy')\n",
    "dict_semart_test_data_loader = TorchDataLoader(dict_semart_test, batch_size=50,\n",
    "                                  collate_fn=lambda x: text_progress2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89f439-5d51-4d68-906f-6333e189342e",
   "metadata": {},
   "source": [
    "#### 2.1 semart, multiple captions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92274535-f856-404d-a5ed-04165fc33bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f70735d40b8>\n",
      "multiple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 13/13 [00:22<00:00,  1.73s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.1238723982265556,\n",
       "  0.04377896610833391,\n",
       "  0.017749577060783853,\n",
       "  0.008296285553807443],\n",
       " 'METEOR': 0.03853641775444528,\n",
       " 'ROUGE': 0.1403943502673313,\n",
       " 'CIDEr': 0.026411970140918662}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# off-the-shelf model\n",
    "data = torch.load('saved_models/saved_models_grid_m2rst/grid_m2_rst_last.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c05f475c-96fa-4e9a-80a7-6651b6f09b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f70735d40b8>\n",
      "multiple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 13/13 [00:22<00:00,  1.71s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.12809294283327557,\n",
       "  0.045397722109473716,\n",
       "  0.01791234797525151,\n",
       "  0.007650384719347912],\n",
       " 'METEOR': 0.039360563038510016,\n",
       " 'ROUGE': 0.142495980976305,\n",
       " 'CIDEr': 0.020916017474374143}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# off-the-shelf model\n",
    "data = torch.load('saved_models/saved_models_grid_m2rst/grid_m2_rst_best.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae06fdd2-5acf-4d8a-8c2d-16f1a103275a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f70735d40b8>\n",
      "multiple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 13/13 [00:22<00:00,  1.70s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.13904005551409623,\n",
       "  0.04929343466036093,\n",
       "  0.019266916799502324,\n",
       "  0.008159874882971943],\n",
       " 'METEOR': 0.03898631586267387,\n",
       " 'ROUGE': 0.13827194208741386,\n",
       " 'CIDEr': 0.02001249028494681}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# off-the-shelf model\n",
    "data = torch.load('saved_models/saved_models_grid_m2rst/grid_m2_rst_epoch41.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd055763-d596-466d-b2c3-100dbae7681a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc78ceb3-4224-4fb2-ab10-ab7d91d0200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on SemArt    remove <unk>\n",
    "data = torch.load('saved_models/saved_models_saft_grid_m2rst/sa_gridm2rst_sa_last_17epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d100c4c-a934-4eb4-a6cf-b6647291a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on SemArt          remove <unk>\n",
    "data = torch.load('saved_models/saved_models_saft_grid_m2rst/sa_gridm2rst_sa_best_13epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dec64d9-cf7d-4b69-91ee-a5160acb3fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on SemArt             remove <unk>\n",
    "data = torch.load('saved_models/saved_models_saft_grid_m2rst/sa_gridm2rst_sa_last_15epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c10cbf05-226b-4355-ab14-e79e760ee4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on SemArt            remove <unk>\n",
    "data = torch.load('saved_models/saved_models_saft_grid_m2rst/sa_gridm2rst_sa_last_16epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216b1b83-7245-485e-b7a9-dbfe7559c33c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfefa08-88e2-4c35-a542-ac20b1436274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0882d6-f4e2-4fda-8ab0-5d22dfe7f639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eac3d9dd-b0c1-4986-a942-f9e7cd2c0500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on SemArt    remove <unk>\n",
    "data = torch.load('saved_models/saved_models_saft_grid_m2rst/sa_gridm2rst_sa_last_17epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a63f876-4bc2-45ed-af4c-c8bbd34b3f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on SemArt          remove <unk>\n",
    "data = torch.load('saved_models/saved_models_saft_grid_m2rst/sa_gridm2rst_sa_best_13epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14495fc8-d4ad-4886-9c33-7722a9886039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on SemArt             remove <unk>\n",
    "data = torch.load('saved_models/saved_models_saft_grid_m2rst/sa_gridm2rst_sa_last_15epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0015e8ea-b1c5-4d4f-b7f7-b7a39a0eb809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on SemArt            remove <unk>\n",
    "data = torch.load('saved_models/saved_models_saft_grid_m2rst/sa_gridm2rst_sa_last_16epoch.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e5605-c1f5-42b0-9344-70f48e9d062b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8abf39-ea06-4978-8fd3-1bc15093fb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eefa491-283b-4256-b0e7-99fef3db4521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db073bb-04fa-41fd-b609-3172ed25941f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
