{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c816ed50-2f46-4b32-92b0-cc76fbaa4e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import h5py\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7347eb0-30ca-4237-a075-773b5fa2854c",
   "metadata": {},
   "source": [
    "### generate hdf5 file for ArtPedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2054fdd5-ac59-4952-8482-3c87f2b92b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original json file of artpedia dataset \n",
    "file_path = \"../Dataset/artpedia/artpedia.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    artpedia = json.load(f)\n",
    "    \n",
    "roi_path = \"../Dataset/artpedia/artpedia_features/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6821edd-7628-4369-9042-575505ef825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_my2ap = np.load('../Dataset/artpedia/idx_my2ap.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2acadf4b-6f9c-43e8-a9a2-f3d2d3c6ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_idx = sorted([int(name[:-4]) for name in os.listdir(roi_path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "883274c0-357a-4786-bb44-4fb8a14eb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roi_caption_pair(idx):\n",
    "    roi = np.load(roi_path + str(my_idx[idx])+'.npz')['x']\n",
    "    fst_cap = artpedia[str(idx_my2ap[my_idx[idx]])]['visual_sentences'][0]\n",
    "    return roi, fst_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e25cceda-88e4-46f9-9890-ab35f151367d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n"
     ]
    }
   ],
   "source": [
    "ap_roi_cap = h5py.File(\"../Dataset/artpedia/roi_feats.hdf5\", \"w\")\n",
    "\n",
    "for i in range(len(my_idx)):\n",
    "    if (i%200==0):\n",
    "        print(i)\n",
    "    roi, cap = get_roi_caption_pair(i)\n",
    "    ap_roi_cap.create_dataset(f\"{my_idx[i]}_cap\", data=cap)\n",
    "    ap_roi_cap.create_dataset(f\"{my_idx[i]}_features\", data=roi)\n",
    "\n",
    "ap_roi_cap.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87cb7e95-b414-48fb-86d2-19fb882f3d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_roi_cap.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b97204-f139-49ff-854b-b9e6ce47cd8b",
   "metadata": {},
   "source": [
    "### img2captions for semart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c4a2e3-1082-4d57-90c4-0ae434bc2233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"['00097-still_fr.jpg']\", \"['00123-marketse.jpg']\",\n",
       "       \"['00195-5verospi.jpg']\", \"['00255-v_josefa.jpg']\",\n",
       "       \"['00334-portra1.jpg']\"], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_test = pd.read_csv(\"../Dataset/SemArt/prediction_csvs/semart_test_prediction.csv\")\n",
    "sa_test = sa_test[sa_test['predictioin']==0][['img_name', \"caption\"]]\n",
    "np.unique(sa_test['img_name'].to_numpy())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2b3d7d2-6a8d-43f4-8cfd-ee64d6b93008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1301"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_names = sa_val['img_name'].to_numpy()\n",
    "len(img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4705d920-4382-498a-a995-d111378953fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## img_names = sa_test['img_name'].to_numpy()\n",
    "img_cap_map = defaultdict(list)\n",
    "for it, img_name in enumerate(img_names):\n",
    "    img_name = img_name[2:-6]\n",
    "    img_cap_map[img_name].append(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ddfef01-8f57-4dee-bebb-6acb82b7d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/SemArt/val_img_caps_map.json', 'w') as fp:\n",
    "    json.dump(img_cap_map, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ba39850-ec36-41d4-bf35-dad7170c45d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"../Dataset/SemArt/val_img_caps_map.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b47682-ae22-43f4-bf2a-381a66ef1e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf71d8-7415-4480-905b-de8b47b22514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64f8af-660c-4af2-a74f-57f1af901e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "549b9211-8973-4d94-84a0-7ca56520cfd0",
   "metadata": {},
   "source": [
    "### generate hdf5 file for SemArt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50ddc927-3e0f-4b71-8ad7-85695752229f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>['10998-307david.jpg']</td>\n",
       "      <td>Madame S�riziat is shown in an interior settin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>['10998-307david.jpg']</td>\n",
       "      <td>Her cheeks are ruddy and she carries a recentl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  img_name                                            caption\n",
       "16  ['10998-307david.jpg']  Madame S�riziat is shown in an interior settin...\n",
       "17  ['10998-307david.jpg']  Her cheeks are ruddy and she carries a recentl..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_train = pd.read_csv(\"../Dataset/SemArt/prediction_csvs/semart_val_prediction.csv\")\n",
    "sa_train = sa_train[sa_train['predictioin']==0][['img_name', 'caption']]\n",
    "sa_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92fe1947-28e9-42a2-a342-9208e6f7a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_names = np.unique(sa_train['img_name'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15ee140f-b879-4642-9d2a-ce747b820c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "581"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0d31040c-f238-460f-96f9-dd14caa0e69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "sa_roi_cap = h5py.File(\"../Dataset/SemArt/sa_test_roi.hdf5\", \"w\")\n",
    "for it, img_name in enumerate(test_img_names):\n",
    "    if it % 200 == 0:\n",
    "        print(it)\n",
    "    img_name = img_name[2:-6]\n",
    "    roi_path = \"../Dataset/SemArt/roi/\" + img_name + \".npz\"\n",
    "    roi = np.load(roi_path)['x']\n",
    "    sa_roi_cap.create_dataset(img_name, data=roi)\n",
    "sa_roi_cap.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f5df25a9-4a9f-417f-9da5-381a16e0113f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00124-marketst',\n",
       " '00221-oratory',\n",
       " '00361-flight_e',\n",
       " '00418-4saints2',\n",
       " '00515-mother',\n",
       " '00606-4esther',\n",
       " '00640-port_lad',\n",
       " '00849-35_eucha',\n",
       " '00965-05humili',\n",
       " '00973-0fiesol1']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sa_roi_cap.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2c5ec-9349-4fdc-921f-9df13314b18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63f8e5c9-2761-45f9-97a9-0a8202a1e61e",
   "metadata": {},
   "source": [
    "### generate hdf5 file for SemArt (grid feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cf3448e-d617-4f12-87d5-65e61aba4416",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DataProcessor, self).__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = torch.squeeze(x)    # [1, d, h, w] => [d, h, w] \n",
    "        x = x.permute(1, 2, 0)  # [d, h, w] => [h, w, d]\n",
    "        return x.view(-1, x.size(-1))   # [h*w, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17accf1b-105b-4501-b652-52adb0f8fe2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['19873-1darmst.jpg']</td>\n",
       "      <td>Standing in a scalloped niche with projecting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['19873-1darmst.jpg']</td>\n",
       "      <td>The hooped crown, an allusion to the German im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>['19873-1darmst.jpg']</td>\n",
       "      <td>Before them kneels Anna, the only surviving ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>['19873-1darmst.jpg']</td>\n",
       "      <td>In front of Jakob, in a Raphaelesque triangula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>['19873-1darmst.jpg']</td>\n",
       "      <td>The baby, with curly blonde hair and pudgy che...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 img_name                                            caption\n",
       "1   ['19873-1darmst.jpg']  Standing in a scalloped niche with projecting ...\n",
       "2   ['19873-1darmst.jpg']  The hooped crown, an allusion to the German im...\n",
       "7   ['19873-1darmst.jpg']  Before them kneels Anna, the only surviving ch...\n",
       "9   ['19873-1darmst.jpg']  In front of Jakob, in a Raphaelesque triangula...\n",
       "10  ['19873-1darmst.jpg']  The baby, with curly blonde hair and pudgy che..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_train = pd.read_csv(\"../Dataset/SemArt/prediction_csvs/semart_train_prediction.csv\", delimiter=\"\\t\")\n",
    "sa_train = sa_train[sa_train['prediction']==0][['img_name', 'caption']]\n",
    "sa_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c72990ec-4f83-4db9-9219-819ebde2882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_names = np.unique(sa_train['img_name'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f5818a1-dfd1-442d-8649-9a6f605cf89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10860,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c02e94-bc58-4e04-873a-1e2e3b7b9ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0b99feb-47e1-4107-93f2-b059957943b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n"
     ]
    }
   ],
   "source": [
    "sa_grid_cap.close()\n",
    "sa_grid_cap = h5py.File(\"../Dataset/sa_train_grid.hdf5\", \"w\")\n",
    "\n",
    "processor = DataProcessor()  ';kkk'\n",
    "\n",
    "for it, img_name in enumerate(train_img_names):\n",
    "    if it % 500 == 0:\n",
    "        print(it)\n",
    "    img_name = img_name[2:-6]\n",
    "    grid_path = \"../Dataset/SemArt/semart_grid_feats/\" + img_name + \".npy\"\n",
    "    grid_feat = np.load(grid_path)\n",
    "    grid_feat = processor(torch.tensor(grid_feat))\n",
    "    sa_grid_cap.create_dataset(img_name, data=grid_feat)\n",
    "sa_grid_cap.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d270d-4aae-4ae8-986d-d4645f5a18fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb0dc9-fefb-43ba-bd9b-6e16e155f873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bc2479-316f-44d8-a112-7a611f89bde3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abb88cf-c9f6-4d50-b5bc-b66555d60372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677bbf3-1f4d-4c0a-889c-bacac9c42946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb76be3d-80cd-421b-9761-34f2a11bcf3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffa70b10-fb07-4d0e-be4e-093de8001c76",
   "metadata": {},
   "source": [
    "### generate hdf5 file for ArtPedia  (grid features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c652445b-7844-418a-b440-6eb2ce76eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DataProcessor, self).__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = torch.squeeze(x)    # [1, d, h, w] => [d, h, w] \n",
    "        x = x.permute(1, 2, 0)  # [d, h, w] => [h, w, d]\n",
    "        return x.view(-1, x.size(-1))   # [h*w, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e75bb3-be78-4b16-9834-7af147bf08ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9effa2fd-b298-4d2f-a251-3a925e1ba895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original json file of artpedia dataset \n",
    "file_path = \"../Dataset/artpedia/artpedia.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    artpedia = json.load(f)\n",
    "    \n",
    "grid_path = \"../../hy-nas/Dataset/artpedia/artpedia_grid_feats/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdeef0d5-0dcc-44b3-a34e-cec36d5498cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_my2ap = np.load('../Dataset/artpedia/idx_my2ap.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0384b2e3-7ac6-493f-91aa-3017760541b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_idx = sorted([int(name[:-4]) for name in os.listdir(grid_path)])\n",
    "\n",
    "train_myidx = np.load('../Dataset/artpedia/train_myidx.npy')\n",
    "val_myidx = np.load('../Dataset/artpedia/val_myidx.npy')\n",
    "test_myidx = np.load('../Dataset/artpedia/test_myidx.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed1403e5-efba-414e-b5d4-0febfa76c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid_caption_pair(idx):\n",
    "    grid_feat = np.load(grid_path + str(my_idx[idx])+'.npy')\n",
    "    fst_cap = artpedia[str(idx_my2ap[my_idx[idx]])]['visual_sentences']\n",
    "    return grid_feat, fst_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fc7da53-a954-4db1-9587-26e81e46d6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It is also stylistically earlier to that work, being painted without pseudo-perspective, and having the angels around the Virgin simply placed one above the other, rather than being spatially arranged.', 'The throne is similar to the Maestà painted by Cimabue in the Basilica of San Francesco di Assisi (1288–1292).']\n",
      "---\n",
      "[b'It is also stylistically earlier to that work, being painted without pseudo-perspective, and having the angels around the Virgin simply placed one above the other, rather than being spatially arranged.', b'The throne is similar to the Maest painted by Cimabue in the Basilica of San Francesco di Assisi (12881292).']\n"
     ]
    }
   ],
   "source": [
    "print(get_grid_caption_pair(0)[1])\n",
    "print(\"---\")\n",
    "print([n.encode(\"ascii\", \"ignore\") for n in get_grid_caption_pair(0)[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b6324e-d176-46de-aea3-39e959dfef39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c45d39ef-5ec8-4d79-b329-8f1adfd08999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 19, 19])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 2048])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = DataProcessor()  \n",
    "\n",
    "feat = torch.tensor(get_grid_caption_pair(0)[0])\n",
    "print(feat.shape)\n",
    "\n",
    "pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "processor(feat).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e572736-c884-4a6a-802c-bc6e52645a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3a86452-e42a-463d-a13f-e7639e1583ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2804"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5449fbaa-97e3-4e21-ac92-653ad9ea8f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "300\n",
      "600\n",
      "900\n",
      "1200\n",
      "1500\n",
      "1800\n",
      "2100\n",
      "2400\n",
      "2700\n"
     ]
    }
   ],
   "source": [
    "# train_ap_grid_cap.close()\n",
    "# val_ap_grid_cap.close()\n",
    "# test_ap_grid_cap.close()\n",
    "\n",
    "train_ap_grid_cap = h5py.File(\"..//Dataset/artpedia/ap_train_grid.hdf5\", \"w\")\n",
    "val_ap_grid_cap = h5py.File(\"../Dataset/artpedia/ap_val_grid.hdf5\", \"w\")\n",
    "test_ap_grid_cap = h5py.File(\"../Dataset/artpedia/ap_test_grid.hdf5\", \"w\")\n",
    "\n",
    "processor = DataProcessor()  \n",
    "\n",
    "for i in range(len(my_idx)):\n",
    "    if (i%300==0):\n",
    "        print(i)\n",
    "    grid_feat, cap = get_grid_caption_pair(i)\n",
    "    grid_feat = processor(torch.tensor(grid_feat))\n",
    "    \n",
    "    myidx = my_idx[i]\n",
    "    \n",
    "    cap = [n.encode(\"ascii\", \"ignore\") for n in cap]\n",
    "    \n",
    "    if myidx in train_myidx:\n",
    "#         print(cap, len(cap))\n",
    "        train_ap_grid_cap.create_dataset(f\"{my_idx[i]}_cap\", (len(cap),),  dtype=h5py.special_dtype(vlen=str), data=cap)\n",
    "        train_ap_grid_cap.create_dataset(f\"{my_idx[i]}_grids\", data=grid_feat)\n",
    "    elif myidx in val_myidx:\n",
    "#         print(cap, len(cap))\n",
    "        val_ap_grid_cap.create_dataset(f\"{my_idx[i]}_cap\", (len(cap),),  dtype=h5py.special_dtype(vlen=str), data=cap)\n",
    "        val_ap_grid_cap.create_dataset(f\"{my_idx[i]}_grids\", data=grid_feat)\n",
    "    elif myidx in test_myidx:\n",
    "#         print(cap, len(cap))\n",
    "        test_ap_grid_cap.create_dataset(f\"{my_idx[i]}_cap\", (len(cap),),  dtype=h5py.special_dtype(vlen=str), data=cap)\n",
    "        test_ap_grid_cap.create_dataset(f\"{my_idx[i]}_grids\", data=grid_feat)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "train_ap_grid_cap.close()\n",
    "val_ap_grid_cap.close()\n",
    "test_ap_grid_cap.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26fd5064-39e3-44d9-8e90-2a532a8fd899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0_cap', '0_grids', '1000_cap', '1000_grids', '1001_cap']\n",
      "4316\n",
      "<class 'numpy.ndarray'>\n",
      "['It is also stylistically earlier to that work, being painted without pseudo-perspective, and having the angels around the Virgin simply placed one above the other, rather than being spatially arranged.'\n",
      " 'The throne is similar to the Maest painted by Cimabue in the Basilica of San Francesco di Assisi (12881292).']\n"
     ]
    }
   ],
   "source": [
    "train_ap_grid_cap_r = h5py.File(\"../Dataset/artpedia/ap_train_grid.hdf5\", \"r\")\n",
    "val_ap_grid_cap_r = h5py.File(\"../Dataset/artpedia/ap_val_grid.hdf5\", \"r\")\n",
    "test_ap_grid_cap_r = h5py.File(\"../Dataset/artpedia/ap_test_grid.hdf5\", \"r\")\n",
    "\n",
    "print(list(train_ap_grid_cap_r.keys())[:5])\n",
    "print(len(list(train_ap_grid_cap_r.keys())))\n",
    "\n",
    "print(type(train_ap_grid_cap_r['0_cap'].value))\n",
    "print(train_ap_grid_cap_r['0_cap'].value)\n",
    "\n",
    "# train_ap_grid_cap_r.close()\n",
    "# val_ap_grid_cap_r.close()\n",
    "# test_ap_grid_cap_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa38fda-c5ec-4f7a-a157-93f78ca70c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc2cf2b6-ca77-407d-8940-baa4784b28ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 2048)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ap_grid_cap_r['0_grids'].value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77a33ea6-d86a-488f-9ac4-34a8954d57c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1002_cap', '1002_grids', '1016_cap', '1016_grids', '1026_cap']\n",
      "634\n"
     ]
    }
   ],
   "source": [
    "print(list(val_ap_grid_cap_r.keys())[:5])\n",
    "print(len(list(val_ap_grid_cap_r.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f3dbea0-8bef-477b-a601-1f56c6f964cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1004_cap', '1004_grids', '1006_cap', '1006_grids', '1023_cap']\n",
      "658\n"
     ]
    }
   ],
   "source": [
    "print(list(test_ap_grid_cap_r.keys())[:5])\n",
    "print(len(list(test_ap_grid_cap_r.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f38edcde-b755-47aa-87f3-5fd88fa6ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ap_grid_cap_r.close()\n",
    "val_ap_grid_cap_r.close()\n",
    "test_ap_grid_cap_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0697d49-b46c-4bae-b1d6-3fc55bbeab8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
