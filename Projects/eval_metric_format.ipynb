{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "188733f4-567a-43ca-a309-8c23e3266769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from data import ImageDetectionsField, TextField, RawField\n",
    "from data import COCO, DataLoader\n",
    "from data.dataset import AP_Dataset, APeval_Dataset, SA_Dataset, SAeval_Dataset\n",
    "import evaluation\n",
    "from evaluation import PTBTokenizer, Cider\n",
    "from models.m2 import Transformer, MemoryAugmentedEncoder, MeshedDecoder, ScaledDotProductAttentionMemory\n",
    "# from models.grid_m2 import Transformer, MemoryAugmentedEncoder, MeshedDecoder, ScaledDotProductAttentionMemory\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn import NLLLoss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import argparse, os, pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from shutil import copyfile\n",
    "import h5py\n",
    "from utils import text_progress2, text_progress\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a65ee104-c363-473a-8c6c-56105168407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, dataloader, text_field, mode=\"multiple\", is_sample=False, beam_size=5, top_k=5, top_p=0.8):\n",
    "    import itertools\n",
    "    print(dataloader)\n",
    "    print(mode)\n",
    "    model.eval()\n",
    "    gen = {}\n",
    "    gts = {}\n",
    "    with tqdm(desc='evalulateion metrics', unit='it', total=len(dataloader)) as pbar:\n",
    "        for it, batch in enumerate(iter(dataloader)):\n",
    "            images = batch['roi_feat']\n",
    "            caps_gt = batch['cap']\n",
    "            images = images.to(device)\n",
    "            with torch.no_grad():\n",
    "#                 beam_size = 5\n",
    "                out, _ = model.beam_search(images, 20, text_field.vocab.stoi['<eos>'], beam_size, out_size=1, is_sample=is_sample, top_k=5, top_p=0.8)\n",
    "#                 if decode == \"beam_search\":\n",
    "#                     out, _ = model.beam_search(images, 20, text_field.vocab.stoi['<eos>'], 5, out_size=1, is_sample=False)\n",
    "#                 elif decode == \"top-k_sampling\":\n",
    "#                     out, _ = model.beam_search(images, 20, text_field.vocab.stoi['<eos>'], 1, out_size=1, is_sample=True)\n",
    "            caps_gen = text_field.decode(out, join_words=False)\n",
    "            for i, (gts_i, gen_i) in enumerate(zip(caps_gt, caps_gen)):\n",
    "                gen_i = ' '.join([k for k, g in itertools.groupby(gen_i)])\n",
    "                gen['%d_%d' % (it, i)] = [gen_i, ]\n",
    "                if mode == \"multiple\":\n",
    "                    gts['%d_%d' % (it, i)] = gts_i\n",
    "                elif mode == \"single\":\n",
    "                    gts['%d_%d' % (it, i)] = [gts_i[0]]\n",
    "            pbar.update()\n",
    "\n",
    "    gts = evaluation.PTBTokenizer.tokenize(gts)\n",
    "    gen = evaluation.PTBTokenizer.tokenize(gen)\n",
    "    scores, _ = evaluation.compute_scores(gts, gen, spice=False)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb3c8e-c055-4ac0-aa70-4a312b25b17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61f0b12d-fa33-4443-9498-dbc3aee9d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cae63e52-5369-4e80-9caf-289003b1768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "text_field = TextField(init_token='<bos>', eos_token='<eos>', lower=True, tokenize='spacy',\n",
    "                       remove_punctuation=True, nopoints=False)\n",
    "text_field.vocab = pickle.load(open('vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dcd1541-8fb8-4a81-bf2b-4ea347114e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MemoryAugmentedEncoder(3, 0, attention_module=ScaledDotProductAttentionMemory,\n",
    "                                 attention_module_kwargs={'m': 40})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3202a6ea-5e67-475f-8cd9-77615e0e2b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = MeshedDecoder(len(text_field.vocab), 54, 3, text_field.vocab.stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bde8e93-0ec2-4186-841a-c8fe759ba741",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(text_field.vocab.stoi['<bos>'], encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2700e21-55c7-42ee-9268-75558e696a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "text_field = TextField(init_token='<bos>', eos_token='<eos>', lower=True, tokenize='spacy',\n",
    "                       remove_punctuation=True, nopoints=False)\n",
    "text_field.vocab = pickle.load(open('vocab.pkl', 'rb'))\n",
    "\n",
    "encoder = MemoryAugmentedEncoder(3, 0, attention_module=ScaledDotProductAttentionMemory,\n",
    "                                 attention_module_kwargs={'m': 40})\n",
    "decoder = MeshedDecoder(len(text_field.vocab), 54, 3, text_field.vocab.stoi['<pad>'])\n",
    "\n",
    "model = Transformer(text_field.vocab.stoi['<bos>'], encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6640d7-492b-4377-a09b-29e94979691a",
   "metadata": {},
   "source": [
    "## 1. artpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7cf6de3-3cfd-4dac-a1d6-d64ec41fdad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data: done!!!\n"
     ]
    }
   ],
   "source": [
    "train_myidx = np.load('../Dataset/artpedia/train_myidx.npy')\n",
    "val_myidx = np.load('../Dataset/artpedia/val_myidx.npy')\n",
    "test_myidx = np.load('../Dataset/artpedia/test_myidx.npy')\n",
    "\n",
    "ap_train_dataset = h5py.File(\"../Dataset/artpedia/artpedia_train2.hdf5\", \"r\")\n",
    "ap_val_dataset = h5py.File(\"../Dataset/artpedia/artpedia_val2.hdf5\", \"r\")\n",
    "ap_test_dataset = h5py.File(\"../Dataset/artpedia/artpedia_test2.hdf5\", \"r\")\n",
    "print(\"loading data: done!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "488f8c01-5e00-42d0-8a96-ee2380fac9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artpedia dataset\n",
    "dict_artpedia_test = APeval_Dataset(ap_test_dataset, test_myidx, text_field, max_detections=50, lower=True, remove_punctuation=True, tokenize='spacy')\n",
    "\n",
    "# artpedia, dataloader\n",
    "dict_artpedia_test_data_loader = TorchDataLoader(dict_artpedia_test, batch_size=50, collate_fn=lambda x: text_progress2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1bc509-fce3-4517-aa44-d3e14147af94",
   "metadata": {},
   "source": [
    "#### 1.1 artpedia, one caption for evaluation, beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e75c5b3-306a-4b61-bc43-a250dcba5de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f1660140860>\n",
      "single\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 66/66 [00:13<00:00,  4.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.0001236055972225921,\n",
       "  4.035351405369087e-12,\n",
       "  1.3192351808657412e-14,\n",
       "  7.681117691112694e-16],\n",
       " 'ROUGE': 0.00017328068630514444,\n",
       " 'CIDEr': 2.058933461283359e-06}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scratch model\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode = \"single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "992b45e1-bfea-4aca-956c-eebf95ccdea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f1660140860>\n",
      "single\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 66/66 [00:13<00:00,  4.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.06869365934607014,\n",
       "  0.02408617516886492,\n",
       "  0.009203178196801148,\n",
       "  0.004077820732889246],\n",
       " 'ROUGE': 0.1265856198220996,\n",
       " 'CIDEr': 0.035073845779796387}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without fine-tuning, off-the-shelf model\n",
    "data = torch.load('meshed_memory_transformer.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea105a5e-62bd-464d-be30-1c4c119a600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7faf5db3dcc0>\n",
      "single\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 66/66 [00:13<00:00,  4.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.10770007890173203,\n",
       "  0.042905798133398614,\n",
       "  0.021158617232954074,\n",
       "  0.011994791859808066],\n",
       " 'ROUGE': 0.16777807810792697,\n",
       " 'CIDEr': 0.05152375363409006}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune on artpedia, one image with one caption for training\n",
    "data = torch.load('saved_models/artpedia_finetune_singlecap.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81142695-0557-4842-b6cd-e2a4a15bb0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7faf5db3dcc0>\n",
      "single\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 66/66 [00:13<00:00,  4.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.10001649470859435,\n",
       "  0.04153007097875002,\n",
       "  0.020453615889150106,\n",
       "  0.010804099773927376],\n",
       " 'ROUGE': 0.16959301187547615,\n",
       " 'CIDEr': 0.04574429996778054}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune on artpedia, one image with multiple captions for training\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a82b9e6-8f61-450e-92e0-f32af0a82cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7faf5db3dcc0>\n",
      "single\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 66/66 [00:13<00:00,  4.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.11439772302455756,\n",
       "  0.049840209225196434,\n",
       "  0.023219958693446556,\n",
       "  0.012087941730175456],\n",
       " 'ROUGE': 0.17763442833720808,\n",
       " 'CIDEr': 0.04937305896140482}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune on artpedia, one image with multiple captions for training   + shuffle\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ee71940-0004-4a80-a145-46fb20ac0fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f49562804e0>\n",
      "single\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 329/329 [01:05<00:00,  5.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.20352391671520823,\n",
       "  0.08086088054241472,\n",
       "  0.03645516117706604,\n",
       "  0.017655400779776953],\n",
       " 'ROUGE': 0.18542789079533953,\n",
       " 'CIDEr': 0.07592433158198265}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# beam search.   remove <unk> token\n",
    "# fine-tune on artpedia, one image with multiple captions for training   + shuffle\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf010ed9-db61-4456-a878-f7f005e043cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504081d-3a13-44de-bc1d-e09fbf6c52ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00fc052f-94d8-447c-afca-c58129550c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fd22eadd0f0>\n",
      "single\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 329/329 [01:00<00:00,  5.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.09333995593432404,\n",
       "  0.038172705149673934,\n",
       "  0.01817147245345931,\n",
       "  0.009833963777214086],\n",
       " 'ROUGE': 0.16580195687810742,\n",
       " 'CIDEr': 0.038267756192650784}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune on artpedia, one image with multiple captions for training   + shuffle\n",
    "# generation by top-k sampling    k=5\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='single', is_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb308f-5489-45e8-850d-8749a9394e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf5f1b8-d0c1-4ade-905a-51010c454f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f70e047ff28>\n",
      "single\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 329/329 [01:02<00:00,  5.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.13215944908390478,\n",
       "  0.04599203906981929,\n",
       "  0.015877252152370982,\n",
       "  0.005857381895548809],\n",
       " 'ROUGE': 0.1465819237593196,\n",
       " 'CIDEr': 0.030852426204639675}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune on artpedia, one image with multiple captions for training   + shuffle\n",
    "# generation by top-k sampling    k=10\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='single', decode=\"top-k_sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "150315d5-7205-4f09-9330-f1ff42f9a52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fd22eadd0f0>\n",
      "single\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 329/329 [01:02<00:00,  5.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.1336230293647883,\n",
       "  0.0478548036942355,\n",
       "  0.015488464819317658,\n",
       "  0.005749882420519693],\n",
       " 'ROUGE': 0.1460779368214854,\n",
       " 'CIDEr': 0.027999531132453754}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune on artpedia, one image with multiple captions for training   + shuffle\n",
    "# generation by top-k sampling    k=10\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='single', is_sample=True, top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c7c6f-b02a-427e-ab1f-312a5ecaebea",
   "metadata": {},
   "source": [
    "#### 1.2 artpedia, multiple captions for evaluation, beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "618c0638-5019-429e-aa75-2102b9bd9272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f1660140860>\n",
      "multiple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 66/66 [00:13<00:00,  4.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.0012174750562701938,\n",
       "  1.4052652310529447e-11,\n",
       "  3.248509699697838e-14,\n",
       "  1.5904823867729776e-15],\n",
       " 'ROUGE': 0.0014741008774548938,\n",
       " 'CIDEr': 0.00020410413642154462}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scratch model\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5306404-06f2-4fbe-a43b-03bc5725f87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f1660140860>\n",
      "multiple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 66/66 [00:13<00:00,  4.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.17192803131078616,\n",
       "  0.07112076525833422,\n",
       "  0.028359206677696855,\n",
       "  0.012836546820123028],\n",
       " 'ROUGE': 0.16342891844085322,\n",
       " 'CIDEr': 0.027290209662590086}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without fine-tuning, off-the-shelf model\n",
    "data = torch.load('meshed_memory_transformer.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed8a3825-0fd6-4bcc-be54-e7697234b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f1660140860>\n",
      "multiple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 66/66 [00:13<00:00,  4.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.2263770796898909,\n",
       "  0.10189771635005827,\n",
       "  0.04723688221258288,\n",
       "  0.024825597404249923],\n",
       " 'ROUGE': 0.2044362212159059,\n",
       " 'CIDEr': 0.03947330064255181}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune on artpedia, one image with one caption for training\n",
    "data = torch.load('saved_models/artpedia_finetune_singlecap.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1960b58e-52ca-4c41-92ab-0298ef20a7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f1660140860>\n",
      "multiple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 66/66 [00:13<00:00,  4.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.21496088729295892,\n",
       "  0.10277365798269977,\n",
       "  0.04822575058710956,\n",
       "  0.024778542489085762],\n",
       " 'ROUGE': 0.2091605244824866,\n",
       " 'CIDEr': 0.03343695862148488}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune on artpedia, one image with multiple captions for training\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e9c7f89-3625-4677-a1f2-ae64d2a85ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fd22eadd0f0>\n",
      "multiple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 329/329 [01:03<00:00,  5.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.23290164145038766,\n",
       "  0.11628312648636245,\n",
       "  0.052702077361140964,\n",
       "  0.02689678159915748],\n",
       " 'ROUGE': 0.21914811563939154,\n",
       " 'CIDEr': 0.03148247854508848}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune on artpedia, one image with multiple captions for training      shuffle\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfac020-93e3-4a39-8809-d88e635b088c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5d42a19-ff8c-4647-bc54-d3e765365d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f49562804e0>\n",
      "multiple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 329/329 [01:05<00:00,  5.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.33960716243253547,\n",
       "  0.16180978856929548,\n",
       "  0.0747107343165195,\n",
       "  0.035472046837868663],\n",
       " 'ROUGE': 0.22451221642237176,\n",
       " 'CIDEr': 0.06145132500410184}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# beam search   remove unk\n",
    "# fine-tune on artpedia, one image with multiple captions for training      shuffle\n",
    "data = torch.load('saved_models/artpedia_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_artpedia_test_data_loader, text_field, is_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bfe0e8-bbed-4c5e-8886-c19de592dc24",
   "metadata": {},
   "source": [
    "## 2. semart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63624600-04f8-419c-b8f0-10c6942c8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_test_csv = pd.read_csv(\"../Dataset/SemArt/prediction_csvs/semart_test_prediction.csv\")\n",
    "sa_test_csv = sa_test_csv[sa_test_csv['predictioin']==0]\n",
    "test_roi_feats = h5py.File(\"../Dataset/SemArt/sa_test_roi.hdf5\", \"r\")\n",
    "test_img_names = np.unique(sa_test_csv['img_name'].to_numpy())\n",
    "test_img_caps_map = json.load(open('../Dataset/SemArt/test_img_caps_map.json'))\n",
    "\n",
    "dict_semart_test = SAeval_Dataset(sa_test_csv, test_img_names, test_img_caps_map, test_roi_feats, text_field, max_detections=50, lower=True, remove_punctuation=True, tokenize='spacy')\n",
    "dict_semart_test_data_loader = TorchDataLoader(dict_semart_test, batch_size=10,\n",
    "                                  collate_fn=lambda x: text_progress2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89f439-5d51-4d68-906f-6333e189342e",
   "metadata": {},
   "source": [
    "#### 2.1 semart, multiple captions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02caa7c7-c3e0-4823-8578-f577c524a98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7faf5de78cf8>\n",
      "multiple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 62/62 [00:15<00:00,  4.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.0018378225188191279,\n",
       "  1.1993886603098599e-11,\n",
       "  2.291862529361248e-14,\n",
       "  1.0199533973959484e-15],\n",
       " 'ROUGE': 0.0020635112957342753,\n",
       " 'CIDEr': 0.0003285390689160264}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scratch model\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92274535-f856-404d-a5ed-04165fc33bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7faf5de78cf8>\n",
      "multiple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 62/62 [00:15<00:00,  4.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.11661231133981016,\n",
       "  0.04033414573061459,\n",
       "  0.015035969453543152,\n",
       "  0.006403400474171556],\n",
       " 'ROUGE': 0.14120449334790547,\n",
       " 'CIDEr': 0.017936319393811125}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# off-the-shelf model\n",
    "data = torch.load('meshed_memory_transformer.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eac3d9dd-b0c1-4986-a942-f9e7cd2c0500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7faf5de78cf8>\n",
      "multiple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 62/62 [00:15<00:00,  4.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.1943147719540551,\n",
       "  0.09214780595267547,\n",
       "  0.04630569693062932,\n",
       "  0.02707906791756692],\n",
       " 'ROUGE': 0.21633784481124718,\n",
       " 'CIDEr': 0.057332406384715213}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune on SemArt\n",
    "data = torch.load('saved_models/semart_finetune_mulcap.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19ed47e2-8b1e-4e05-84d2-1ee40275b7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7faf5de78cf8>\n",
      "multiple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 62/62 [00:15<00:00,  4.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.19465401469523047,\n",
       "  0.09556125592848534,\n",
       "  0.047773953419375625,\n",
       "  0.028291138877741047],\n",
       " 'ROUGE': 0.2129567245656028,\n",
       " 'CIDEr': 0.05563681242748405}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune on SemArt   shuffle\n",
    "data = torch.load('saved_models/semart_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a63f876-4bc2-45ed-af4c-c8bbd34b3f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95265f56-8b8d-4536-a26e-ccbc05c225b1",
   "metadata": {},
   "source": [
    "#### 2.2 semart, single captions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "280720ad-4915-4249-a8c7-fc1a54f05014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7faf5de78cf8>\n",
      "single\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 62/62 [00:29<00:00,  2.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.0003956737196290604,\n",
       "  4.901831183185136e-12,\n",
       "  1.1638071419966104e-14,\n",
       "  5.789889675096696e-16],\n",
       " 'ROUGE': 0.0004424504289318356,\n",
       " 'CIDEr': 3.8845498212968275e-05}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scratch model\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd75e22f-d330-4200-84b5-77888610af34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7faf5de78cf8>\n",
      "single\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 62/62 [00:25<00:00,  2.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.056891952408472155,\n",
       "  0.01754322385391535,\n",
       "  0.00642865973036798,\n",
       "  0.002323683418409561],\n",
       " 'ROUGE': 0.1162408217732889,\n",
       " 'CIDEr': 0.024121582330860605}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# off-the-shelf model\n",
    "data = torch.load('meshed_memory_transformer.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54fc5968-92d3-4f67-a23c-a8c952cfa7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7faf5de78cf8>\n",
      "single\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 62/62 [00:29<00:00,  2.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.11726016251663722,\n",
       "  0.0530377317106873,\n",
       "  0.02759006235809311,\n",
       "  0.01663845349934508],\n",
       " 'ROUGE': 0.18710581654278827,\n",
       " 'CIDEr': 0.07783775341498904}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune on SemArt\n",
    "data = torch.load('saved_models/semart_finetune_mulcap.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a636953e-6ac2-403c-afbf-0be77ea279a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7faf5de78cf8>\n",
      "single\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalulateion metrics: 100%|██████████| 62/62 [00:15<00:00,  4.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': [0.12102475253065365,\n",
       "  0.05635952468793378,\n",
       "  0.02944552781687917,\n",
       "  0.01802454846235869],\n",
       " 'ROUGE': 0.18495195667006736,\n",
       " 'CIDEr': 0.07142283831893503}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune on SemArt   shuffle\n",
    "data = torch.load('saved_models/semart_finetune_mulcap_shuffle.pth')\n",
    "model.load_state_dict(data['state_dict'])\n",
    "evaluate_metrics(model, dict_semart_test_data_loader, text_field, mode='single')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2release",
   "language": "python",
   "name": "m2release"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
